{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distilgpt2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RUhn_Cn8apS",
        "outputId": "1098594f-b3fb-4ee5-9bc2-208be1ff61b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.1-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.7 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "oP1n4UeQXKc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70jKKVqh8EBv",
        "outputId": "7d24a485-e10f-4d91-a2a9-99fa337a7d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "tokenizer.save_pretrained('./local_model/')\n",
        "model.save_pretrained('./local_model/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('./local_model/')\n",
        "\n",
        "model = TFGPT2LMHeadModel.from_pretrained('./local_model/',pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "tokenizer.decode(tokenizer.eos_token_id)\n",
        "\n",
        "sentence = \"Hello, my dog is cute\"\n",
        "input_ids = tokenizer.encode(sentence, return_tensors='tf')\n",
        "\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=500,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "print(\"-----\")\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6r03PCXJVfW",
        "outputId": "3f3c6678-1537-4a84-daaf-03a19378b104"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./local_model/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "Hello, my dog is cute, and I love to play with it.\n",
            "\n",
            "I’m so happy to be able to share this with you!\n",
            "Thank you so much for reading!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs[wizard]"
      ],
      "metadata": {
        "id": "G0hYaKELCVgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorflowjs_converter \\\n",
        "    --input_format=keras \\\n",
        "    ./local_model/tf_model.h5 \\\n",
        "    ./distilgpt2-tfjs"
      ],
      "metadata": {
        "id": "gtBU3adpCheF"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ./distilgpt2-tfjs.zip ./distilgpt2-tfjs/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2Sjgim2DoAs",
        "outputId": "f8533a55-e4e6-46a4-cc44-3a666e86a4f3"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: distilgpt2-tfjs/ (stored 0%)\n",
            "  adding: distilgpt2-tfjs/group1-shard38of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard50of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard59of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard60of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard11of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard10of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard17of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard65of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard76of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard2of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard8of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard57of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard26of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard62of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard72of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard9of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard15of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard25of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard49of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard6of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard20of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard33of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard56of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard41of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard32of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard55of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard40of79.bin (deflated 6%)\n",
            "  adding: distilgpt2-tfjs/group1-shard18of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard36of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard4of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard42of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/model.json (deflated 93%)\n",
            "  adding: distilgpt2-tfjs/group1-shard29of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard13of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard63of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard79of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard7of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard14of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard24of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard30of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard64of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard58of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard43of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard75of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard37of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard66of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard22of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard27of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard78of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard35of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard39of79.bin (deflated 6%)\n",
            "  adding: distilgpt2-tfjs/group1-shard45of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard70of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard46of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard1of79.bin (deflated 6%)\n",
            "  adding: distilgpt2-tfjs/group1-shard51of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard31of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard71of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard77of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard74of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard44of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard5of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard12of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard34of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard48of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard19of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard69of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard52of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard73of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard28of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard67of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard3of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard23of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard61of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard53of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard54of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard16of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard68of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard47of79.bin (deflated 7%)\n",
            "  adding: distilgpt2-tfjs/group1-shard21of79.bin (deflated 7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyinstaller"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jah_VbBtYxbg",
        "outputId": "bc95b59a-04a4-4360-f5fd-16343378584f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyinstaller in /usr/local/lib/python3.7/dist-packages (4.8)\n",
            "Requirement already satisfied: pyinstaller-hooks-contrib>=2020.6 in /usr/local/lib/python3.7/dist-packages (from pyinstaller) (2022.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from pyinstaller) (4.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyinstaller) (57.4.0)\n",
            "Requirement already satisfied: altgraph in /usr/local/lib/python3.7/dist-packages (from pyinstaller) (0.17.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->pyinstaller) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->pyinstaller) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys ; sys.setrecursionlimit(sys.getrecursionlimit() * 2000)"
      ],
      "metadata": {
        "id": "PEE5LUX0ZQXD"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"distilgpt2.py\", \"w\")\n",
        "\n",
        "f.write(\"\"\"\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "import tensorflow as tf\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('./local_model/')\n",
        "\n",
        "model = TFGPT2LMHeadModel.from_pretrained('./local_model/',pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "tokenizer.decode(tokenizer.eos_token_id)\n",
        "\n",
        "\n",
        "sentence = \"Hello, my dog is cute\"\n",
        "input_ids = tokenizer.encode(sentence, return_tensors='tf')\n",
        "\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=500,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "\"\"\")\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "6rXXWZVyaTHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pyinstaller -F distilgpt2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC-rQwoRZCRP",
        "outputId": "7311f910-01c2-43f7-cf39-5421faff5466"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76 INFO: PyInstaller: 4.8\n",
            "76 INFO: Python: 3.7.12\n",
            "76 INFO: Platform: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\n",
            "76 INFO: wrote /content/distilgpt2.spec\n",
            "79 INFO: UPX is not available.\n",
            "81 INFO: Extending PYTHONPATH with paths\n",
            "['/content']\n",
            "444 INFO: checking Analysis\n",
            "444 INFO: Building Analysis because Analysis-00.toc is non existent\n",
            "445 INFO: Initializing module dependency graph...\n",
            "450 INFO: Caching module graph hooks...\n",
            "466 INFO: Analyzing base_library.zip ...\n",
            "3185 INFO: Processing pre-find module path hook distutils from '/usr/local/lib/python3.7/dist-packages/PyInstaller/hooks/pre_find_module_path/hook-distutils.py'.\n",
            "3186 INFO: distutils: retargeting to non-venv dir '/usr/lib/python3.7'\n",
            "5220 INFO: Caching module dependency graph...\n",
            "5352 INFO: running Analysis Analysis-00.toc\n",
            "5474 INFO: Analyzing /content/distilgpt2.py\n",
            "11261 INFO: Processing pre-safe import module hook six.moves from '/usr/local/lib/python3.7/dist-packages/PyInstaller/hooks/pre_safe_import_module/hook-six.moves.py'.\n",
            "12535 INFO: Processing pre-find module path hook site from '/usr/local/lib/python3.7/dist-packages/PyInstaller/hooks/pre_find_module_path/hook-site.py'.\n",
            "12535 INFO: site: retargeting to fake-dir '/usr/local/lib/python3.7/dist-packages/PyInstaller/fake-modules'\n",
            "18279 INFO: Processing pre-safe import module hook urllib3.packages.six.moves from '/usr/local/lib/python3.7/dist-packages/PyInstaller/hooks/pre_safe_import_module/hook-urllib3.packages.six.moves.py'.\n",
            "44357 INFO: Processing pre-safe import module hook gi.repository.GObject from '/usr/local/lib/python3.7/dist-packages/PyInstaller/hooks/pre_safe_import_module/hook-gi.repository.GObject.py'.\n",
            "44359 INFO: Processing pre-safe import module hook gi.repository.Gtk from '/usr/local/lib/python3.7/dist-packages/PyInstaller/hooks/pre_safe_import_module/hook-gi.repository.Gtk.py'.\n",
            "61911 INFO: Processing pre-safe import module hook win32com from '/usr/local/lib/python3.7/dist-packages/_pyinstaller_hooks_contrib/hooks/pre_safe_import_module/hook-win32com.py'.\n",
            "61922 INFO: Processing pre-safe import module hook gi.repository.Gio from '/usr/local/lib/python3.7/dist-packages/PyInstaller/hooks/pre_safe_import_module/hook-gi.repository.Gio.py'.\n",
            "\n",
            "=============================================================\n",
            "A RecursionError (maximum recursion depth exceeded) occurred.\n",
            "For working around please follow these instructions\n",
            "=============================================================\n",
            "\n",
            "1. In your program's .spec file add this line near the top::\n",
            "\n",
            "     import sys ; sys.setrecursionlimit(sys.getrecursionlimit() * 5)\n",
            "\n",
            "2. Build your program by running PyInstaller with the .spec file as\n",
            "   argument::\n",
            "\n",
            "     pyinstaller myprog.spec\n",
            "\n",
            "3. If this fails, you most probably hit an endless recursion in\n",
            "   PyInstaller. Please try to track this down has far as possible,\n",
            "   create a minimal example so we can reproduce and open an issue at\n",
            "   https://github.com/pyinstaller/pyinstaller/issues following the\n",
            "   instructions in the issue template. Many thanks.\n",
            "\n",
            "Explanation: Python's stack-limit is a safety-belt against endless recursion,\n",
            "eating up memory. PyInstaller imports modules recursively. If the structure\n",
            "how modules are imported within your program is awkward, this leads to the\n",
            "nesting being too deep and hitting Python's stack-limit.\n",
            "\n",
            "With the default recursion limit (1000), the recursion error occurs at about\n",
            "115 nested imported, with limit 2000 at about 240, with limit 5000 at about\n",
            "660.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}